{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW7FhavoOT6Y"
      },
      "source": [
        "Lab 3\n",
        "1. Ensemble methods\n",
        "    - Bagging\n",
        "    - Boosting\n",
        "    - Random Forests\n",
        "2. Hyperparameter Tuning\n",
        "3. Final System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "9TYMTIKOF9R2"
      },
      "outputs": [],
      "source": [
        "from random import seed\n",
        "from random import randrange\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import model_selection\n",
        "from sklearn.base import clone\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "9JmMh4TQO6F0",
        "outputId": "e572e6aa-598f-412b-e92a-67568850ecbb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>systolic</th>\n",
              "      <th>eyesight(left)</th>\n",
              "      <th>hearing(right)</th>\n",
              "      <th>ALT</th>\n",
              "      <th>relaxation</th>\n",
              "      <th>Cholesterol</th>\n",
              "      <th>AST</th>\n",
              "      <th>hearing(left)</th>\n",
              "      <th>smoking</th>\n",
              "      <th>serum creatinine</th>\n",
              "      <th>Gtp</th>\n",
              "      <th>serum creatinine^2</th>\n",
              "      <th>Gtp^2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1.024253</td>\n",
              "      <td>-1.604784</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.024448</td>\n",
              "      <td>1.171176</td>\n",
              "      <td>-0.840233</td>\n",
              "      <td>-0.393159</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.634722</td>\n",
              "      <td>-0.292828</td>\n",
              "      <td>0.402873</td>\n",
              "      <td>0.085748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.905463</td>\n",
              "      <td>-1.282285</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.183166</td>\n",
              "      <td>0.718083</td>\n",
              "      <td>-0.054246</td>\n",
              "      <td>0.310289</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.211355</td>\n",
              "      <td>0.181165</td>\n",
              "      <td>1.467380</td>\n",
              "      <td>0.032821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.337618</td>\n",
              "      <td>-1.927283</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.451707</td>\n",
              "      <td>-0.188104</td>\n",
              "      <td>-0.625873</td>\n",
              "      <td>0.310289</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.518542</td>\n",
              "      <td>0.939554</td>\n",
              "      <td>0.268886</td>\n",
              "      <td>0.882761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.703813</td>\n",
              "      <td>1.620206</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.134270</td>\n",
              "      <td>1.284450</td>\n",
              "      <td>-0.554420</td>\n",
              "      <td>-0.674538</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.634722</td>\n",
              "      <td>-0.150630</td>\n",
              "      <td>0.402873</td>\n",
              "      <td>0.022689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-0.097288</td>\n",
              "      <td>1.620206</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.976758</td>\n",
              "      <td>-0.074830</td>\n",
              "      <td>-1.447587</td>\n",
              "      <td>-0.815227</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.518542</td>\n",
              "      <td>-0.766821</td>\n",
              "      <td>0.268886</td>\n",
              "      <td>0.588015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  systolic  eyesight(left)  hearing(right)       ALT  relaxation  \\\n",
              "0           0  1.024253       -1.604784             1.0 -0.024448    1.171176   \n",
              "1           1  1.905463       -1.282285             2.0 -0.183166    0.718083   \n",
              "2           2 -0.337618       -1.927283             1.0  0.451707   -0.188104   \n",
              "3           3  0.703813        1.620206             1.0  0.134270    1.284450   \n",
              "4           4 -0.097288        1.620206             1.0 -0.976758   -0.074830   \n",
              "\n",
              "   Cholesterol       AST  hearing(left)  smoking  serum creatinine       Gtp  \\\n",
              "0    -0.840233 -0.393159            1.0      1.0          0.634722 -0.292828   \n",
              "1    -0.054246  0.310289            2.0      0.0          1.211355  0.181165   \n",
              "2    -0.625873  0.310289            1.0      1.0         -0.518542  0.939554   \n",
              "3    -0.554420 -0.674538            1.0      0.0          0.634722 -0.150630   \n",
              "4    -1.447587 -0.815227            1.0      1.0         -0.518542 -0.766821   \n",
              "\n",
              "   serum creatinine^2     Gtp^2  \n",
              "0            0.402873  0.085748  \n",
              "1            1.467380  0.032821  \n",
              "2            0.268886  0.882761  \n",
              "3            0.402873  0.022689  \n",
              "4            0.268886  0.588015  "
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"output.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "K6WzrET4S4xm"
      },
      "outputs": [],
      "source": [
        "X = df.drop('smoking', axis=1)\n",
        "y = df['smoking']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbUVQb2STC4Z",
        "outputId": "af5af86a-3c5d-4c65-93cc-ab7dc0cdc9a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(127158, 13)\n",
            "(31790, 13)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwZVToeeOx0m"
      },
      "source": [
        "# **1. Bagging (Bootstrap Aggregating)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EM_FlMZFYcc"
      },
      "source": [
        "Resources: [Bagging](https://insidelearningmachines.com/build-a-bagging-classifier-in-python/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG8ZDwN_o-Os"
      },
      "source": [
        "Bootstrapping is a statistical method to create sample data without leaving the properties of the actual dataset. The individual samples of data called bootstrap samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGQkJZpAp3ph"
      },
      "source": [
        "## Steps:\n",
        "\n",
        "1. Produce N bootstrap samples on the training data\n",
        "2. Loop through each of the i = 1 -> N bootstrap samples:\n",
        "  - Fit a model to sample i\n",
        "  - Produce the desired predictions with this model.\n",
        "  - Repeat the above two steps, storing the trained models and predictions\n",
        "4. Aggregate the predictions. In the event of having a labelled test set, compare these results with the test dataset labels\n",
        "5. If the results are good, we can deploy our trained ensemble. Input data is provided to the ensemble, and each constituent model produces predictions. These predictions are aggregated to yield a final result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwBXYiDGu0TH"
      },
      "source": [
        "## Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "JoUBDCOeu4X_"
      },
      "outputs": [],
      "source": [
        "n_estimators = 10  # Number of bagging iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "TKl9Jh-wnRtQ"
      },
      "outputs": [],
      "source": [
        "class Bagging():\n",
        "    '''Bagging Classifier from Scratch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_estimators : int\n",
        "        number of bagging iterations\n",
        "    '''\n",
        "\n",
        "    def __init__(self, n_estimators):\n",
        "        self.n_elements = n_estimators\n",
        "        self.models = []\n",
        "\n",
        "    # Private function to make bootstrap samples\n",
        "    def __make_bootstraps(self, data):\n",
        "        # Initialize output dictionary & unique value count\n",
        "        dc = {}\n",
        "        unip = 0\n",
        "        # Get sample size\n",
        "        b_size = data.shape[0]\n",
        "        # Get list of row indexes\n",
        "        idx = [i for i in range(b_size)]\n",
        "        # Loop through the required number of bootstraps\n",
        "        for b in range(self.n_elements):\n",
        "            # Obtain bootstrap samples with replacement\n",
        "            sidx = np.random.choice(idx, replace=True, size=b_size)\n",
        "            b_samp = data[sidx, :]\n",
        "            # Compute number of unique values contained in the bootstrap sample\n",
        "            unip += len(set(sidx))\n",
        "            # Obtain out-of-bag samples for the current bootstrap\n",
        "            oidx = list(set(idx) - set(sidx))\n",
        "            o_samp = np.array([])\n",
        "            if oidx:\n",
        "                o_samp = data[oidx, :]\n",
        "            # Store results\n",
        "            dc['boot_' + str(b)] = {'boot': b_samp, 'test': o_samp}\n",
        "        # Return the bootstrap results\n",
        "        return dc\n",
        "\n",
        "    # Train the ensemble\n",
        "    def fit(self, X_train, y_train, print_metrics=False):\n",
        "      # Convert y_train to a NumPy array\n",
        "      training_data = np.concatenate((X_train, np.array(y_train).reshape(-1, 1)), axis=1)\n",
        "      # Make bootstrap samples\n",
        "      dcBoot = self.__make_bootstraps(training_data)\n",
        "      # Initialize metric arrays\n",
        "      accs = np.array([])\n",
        "      pres = np.array([])\n",
        "      recs = np.array([])\n",
        "      # Iterate through each bootstrap sample & fit a model\n",
        "      cls = DecisionTreeClassifier(class_weight='balanced')\n",
        "      for b in dcBoot:\n",
        "          # Make a clone of the model\n",
        "          model = clone(cls)\n",
        "          # Fit a decision tree classifier to the current sample\n",
        "          model.fit(dcBoot[b]['boot'][:, :-1], dcBoot[b]['boot'][:, -1].reshape(-1, 1))\n",
        "          # Append the fitted model\n",
        "          self.models.append(model)\n",
        "          # Compute the predictions on the out-of-bag test set & compute metrics\n",
        "          if dcBoot[b]['test'].size:\n",
        "              yp = model.predict(dcBoot[b]['test'][:, :-1])\n",
        "              acc = accuracy_score(dcBoot[b]['test'][:, -1], yp)\n",
        "              pre = precision_score(dcBoot[b]['test'][:, -1], yp)\n",
        "              rec = recall_score(dcBoot[b]['test'][:, -1], yp)\n",
        "              # Store the error metrics\n",
        "              accs = np.concatenate((accs, [acc]))\n",
        "              pres = np.concatenate((pres, [pre]))\n",
        "              recs = np.concatenate((recs, [rec]))\n",
        "\n",
        "    # Predict from the ensemble\n",
        "    def predict(self, X):\n",
        "        # Check we've fit the ensemble\n",
        "        if not self.models:\n",
        "            print('You must train the ensemble before making predictions!')\n",
        "            return None\n",
        "        # Loop through each fitted model\n",
        "        predictions = []\n",
        "        for m in self.models:\n",
        "            # Make predictions on the input X\n",
        "            yp = m.predict(X)\n",
        "            # Append predictions to storage list\n",
        "            predictions.append(yp.reshape(-1, 1))\n",
        "        # Compute the ensemble prediction\n",
        "        ypred = np.round(np.mean(np.concatenate(predictions, axis=1), axis=1)).astype(int)\n",
        "        # Return the prediction\n",
        "        return ypred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwSCuUc7CJZ2"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsBg7XsqsdOZ",
        "outputId": "2165812f-4ea0-4b88-c8eb-96805e528373"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bagging Accuracy: 62.2051%\n",
            "Bagging Precision: 61.1695%\n",
            "Bagging Recall: 62.2051%\n"
          ]
        }
      ],
      "source": [
        "# Convert X_train and X_test to NumPy arrays\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "\n",
        "# Train the Bagging model\n",
        "bagging = Bagging(n_estimators=10)\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging.predict(X_test)\n",
        "\n",
        "# Compute accuracy, precision, and recall\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(f\"Bagging Accuracy: {(accuracy*100):.4f}%\")\n",
        "print(f\"Bagging Precision: {(precision*100):.4f}%\")\n",
        "print(f\"Bagging Recall: {(recall*100):.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXQhQEpsOLqC"
      },
      "source": [
        "# **2. Boosting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9BntPcKPIYG"
      },
      "source": [
        "Resources: [Boosting](https://randomrealizations.com/posts/gradient-boosting-multi-class-classification-from-scratch/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w62MzE1-TK2T"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "mRj1WBl3TKEs"
      },
      "outputs": [],
      "source": [
        "n_estimators = 10  # Number of boosting iterations\n",
        "alpha = 0.01  # learning rate\n",
        "max_depth = 3 # maximum tree depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "w-BobGo1eT1Z"
      },
      "outputs": [],
      "source": [
        "class Boosting():\n",
        "    '''Gradient Boosting Classifier from Scratch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_estimators : int\n",
        "        number of boosting iterations\n",
        "\n",
        "    learning_rate : float\n",
        "        learning rate hyperparameter\n",
        "\n",
        "    max_depth : int\n",
        "        maximum tree depth\n",
        "    '''\n",
        "\n",
        "    def __init__(self, n_estimators, learning_rate, max_depth):\n",
        "        self.n_estimators=n_estimators;\n",
        "        self.learning_rate=learning_rate\n",
        "        self.max_depth=max_depth;\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        '''Fit the GBM\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray of size (number observations, number features)\n",
        "            design matrix\n",
        "\n",
        "        y : ndarray of size (number observations,)\n",
        "            integer-encoded target labels in {0,1,...,k-1}\n",
        "        '''\n",
        "\n",
        "        # Flatten y if it is 2D\n",
        "        if len(y.shape) > 1:\n",
        "            y = y.ravel()  # Flatten to 1D\n",
        "\n",
        "        self.n_classes = pd.Series(y).nunique()\n",
        "        y_ohe = self._one_hot_encode_labels(y)\n",
        "\n",
        "        raw_predictions = np.zeros(shape=y_ohe.shape)\n",
        "        probabilities = self._softmax(raw_predictions)\n",
        "        self.boosters = []\n",
        "        for m in range(self.n_estimators):\n",
        "            class_trees = []\n",
        "            for k in range(self.n_classes):\n",
        "                negative_gradients = self._negative_gradients(y_ohe[:, k], probabilities[:, k])\n",
        "                hessians = self._hessians(probabilities[:, k])\n",
        "                tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "                tree.fit(X, negative_gradients)\n",
        "                self._update_terminal_nodes(tree, X, negative_gradients, hessians)\n",
        "                raw_predictions[:, k] += self.learning_rate * tree.predict(X)\n",
        "                probabilities = self._softmax(raw_predictions)\n",
        "                class_trees.append(tree)\n",
        "            self.boosters.append(class_trees)\n",
        "\n",
        "\n",
        "    def _one_hot_encode_labels(self, y):\n",
        "        if isinstance(y, pd.Series): y = y.values\n",
        "        ohe = OneHotEncoder()\n",
        "        y_ohe = ohe.fit_transform(y.reshape(-1, 1)).toarray()\n",
        "        return y_ohe\n",
        "\n",
        "    def _negative_gradients(self, y_ohe, probabilities):\n",
        "        return y_ohe - probabilities\n",
        "\n",
        "    def _hessians(self, probabilities):\n",
        "        return probabilities * (1 - probabilities)\n",
        "\n",
        "    def _softmax(self, raw_predictions):\n",
        "        numerator = np.exp(raw_predictions)\n",
        "        denominator = np.sum(np.exp(raw_predictions), axis=1).reshape(-1, 1)\n",
        "        return numerator / denominator\n",
        "\n",
        "    def _update_terminal_nodes(self, tree, X, negative_gradients, hessians):\n",
        "        '''Update the terminal node predicted values'''\n",
        "        # terminal node id's\n",
        "        leaf_nodes = np.nonzero(tree.tree_.children_left == -1)[0]\n",
        "        # compute leaf for each sample in ``X``.\n",
        "        leaf_node_for_each_sample = tree.apply(X)\n",
        "        for leaf in leaf_nodes:\n",
        "            samples_in_this_leaf = np.where(leaf_node_for_each_sample == leaf)[0]\n",
        "            negative_gradients_in_leaf = negative_gradients.take(samples_in_this_leaf, axis=0)\n",
        "            hessians_in_leaf = hessians.take(samples_in_this_leaf, axis=0)\n",
        "            val = np.sum(negative_gradients_in_leaf) / np.sum(hessians_in_leaf)\n",
        "            tree.tree_.value[leaf, 0, 0] = val\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        '''Generate probability predictions for the given input data.'''\n",
        "        raw_predictions =  np.zeros(shape=(X.shape[0], self.n_classes))\n",
        "        for k in range(self.n_classes):\n",
        "            for booster in self.boosters:\n",
        "                raw_predictions[:, k] +=self.learning_rate * booster[k].predict(X)\n",
        "        probabilities = self._softmax(raw_predictions)\n",
        "        return probabilities\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''Generate predicted labels (as 1-d array)'''\n",
        "        probabilities = self.predict_proba(X)\n",
        "        return np.argmax(probabilities, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PM2fmSuepTz"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rL9au1oeonx",
        "outputId": "4b7b00df-58dc-43ad-f1b2-1c067cd224b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Boosting Accuracy: 63.2337%\n",
            "Boosting Precision: 62.4718%\n",
            "Boosting Recall: 63.2337%\n"
          ]
        }
      ],
      "source": [
        "boosting = Boosting(n_estimators, alpha, max_depth)\n",
        "boosting.fit(X_train, y_train)\n",
        "y_pred = boosting.predict(X_test)\n",
        "\n",
        "# Compute accuracy, precision, and recall on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(f\"Boosting Accuracy: {(accuracy*100):.4f}%\")\n",
        "print(f\"Boosting Precision: {(precision*100):.4f}%\")\n",
        "print(f\"Boosting Recall: {(recall*100):.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8-zh3U4iQeK"
      },
      "source": [
        "## **3. Random Forests**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHXP19BPipE4"
      },
      "source": [
        "Resources: [Random Forest](https://insidelearningmachines.com/build-a-random-forest-in-python/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PcaBeo_8Hu0g"
      },
      "outputs": [],
      "source": [
        "n_trees = 10 # Number of trees in the forest\n",
        "max_depth = 3 # Maximum depth of the tree\n",
        "min_samples_split = 2 # The minimum number of samples required to split an internal node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Wqr5dpNRiZvR"
      },
      "outputs": [],
      "source": [
        "class RandomForest():\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "class RandomForest():\n",
        "    def __init__(self, n_trees, max_depth, min_samples_split, criterion='gini', balance_class_weights=False):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion = criterion  # Update the parameter name to 'criterion'\n",
        "        self.balance_class_weights = balance_class_weights\n",
        "        self.trees = []\n",
        "\n",
        "    # protected function to obtain the right decision tree\n",
        "    def _make_tree_model(self):\n",
        "        return DecisionTreeClassifier(max_depth=self.max_depth, min_samples_split=self.min_samples_split, criterion=self.criterion, class_weight='balanced' if self.balance_class_weights else None)\n",
        "\n",
        "    # private function to make bootstrap samples\n",
        "    def __make_bootstraps(self,data):\n",
        "        # initialize output dictionary & unique value count\n",
        "        dc = {}\n",
        "        unip = 0\n",
        "        # get sample size\n",
        "        b_size = data.shape[0]\n",
        "        # get list of row indexes\n",
        "        idx = [i for i in range(b_size)]\n",
        "        # loop through the required number of bootstraps\n",
        "        for b in range(self.n_trees):\n",
        "            # obtain boostrap samples with replacement\n",
        "            sidx = np.random.choice(idx,replace=True,size=b_size)\n",
        "            b_samp = data[sidx,:]\n",
        "            #compute number of unique values contained in the bootstrap sample\n",
        "            unip += len(set(sidx))\n",
        "            # obtain out-of-bag samples for the current b\n",
        "            oidx = list(set(idx) - set(sidx))\n",
        "            o_samp = np.array([])\n",
        "            if oidx:\n",
        "                o_samp = data[oidx,:]\n",
        "            #store results\n",
        "            dc['boot_'+str(b)] = {'boot':b_samp,'test':o_samp}\n",
        "        #return the bootstrap results\n",
        "        return(dc)\n",
        "\n",
        "    # protected function to train the ensemble\n",
        "    def _train(self,X_train,y_train):\n",
        "        #package the input data\n",
        "        training_data = np.concatenate((X_train,y_train.reshape(-1,1)),axis=1)\n",
        "        #make bootstrap samples\n",
        "        dcBoot = self.__make_bootstraps(training_data)\n",
        "        #iterate through each bootstrap sample & fit a model ##\n",
        "        tree_m = self._make_tree_model()\n",
        "        dcOob = {}\n",
        "        for b in dcBoot:\n",
        "            # make a clone of the model\n",
        "            model = clone(tree_m)\n",
        "            # fit a decision tree model to the current sample\n",
        "            model.fit(dcBoot[b]['boot'][:,:-1],dcBoot[b]['boot'][:,-1].reshape(-1, 1))\n",
        "            # append the fitted model\n",
        "            self.trees.append(model)\n",
        "            # store the out-of-bag test set for the current bootstrap\n",
        "            if dcBoot[b]['test'].size:\n",
        "                dcOob[b] = dcBoot[b]['test']\n",
        "            else:\n",
        "                dcOob[b] = np.array([])\n",
        "        #return the oob data set\n",
        "        return(dcOob)\n",
        "\n",
        "    # train the ensemble\n",
        "    def fit(self, X_train, y_train,print_metrics=False):\n",
        "        # call the protected training method\n",
        "        dcOob = self._train(X_train,y_train)\n",
        "        # if selected, compute the standard errors and print them\n",
        "        if print_metrics:\n",
        "            # initialise metric arrays\n",
        "            accs = np.array([])\n",
        "            pres = np.array([])\n",
        "            recs = np.array([])\n",
        "            # loop through each bootstrap sample\n",
        "            for b,m in zip(dcOob,self.trees):\n",
        "                # compute the predictions on the out-of-bag test set & compute metrics\n",
        "                if dcOob[b].size:\n",
        "                    yp  = m.predict(dcOob[b][:,:-1])\n",
        "                    acc = accuracy_score(dcOob[b][:,-1],yp)\n",
        "                    pre = precision_score(dcOob[b][:,-1],yp,average='weighted')\n",
        "                    rec = recall_score(dcOob[b][:,-1],yp,average='weighted')\n",
        "\n",
        "    #protected function to predict from the ensemble\n",
        "    def _predict(self,X):\n",
        "        #check we've fit the ensemble\n",
        "        if not self.trees:\n",
        "            print('You must train the ensemble before making predictions!')\n",
        "            return(None)\n",
        "        #loop through each fitted model\n",
        "        predictions = []\n",
        "        for m in self.trees:\n",
        "            #make predictions on the input X\n",
        "            yp = m.predict(X)\n",
        "            #append predictions to storage list\n",
        "            predictions.append(yp.reshape(-1,1))\n",
        "        #compute the ensemble prediction\n",
        "        ypred = np.mean(np.concatenate(predictions,axis=1),axis=1)\n",
        "        #return the prediction\n",
        "        return(ypred)\n",
        "\n",
        "    # predict from the ensemble\n",
        "    def predict(self,X):\n",
        "        # call the protected prediction method\n",
        "        ypred = self._predict(X)\n",
        "        # convert the results into integer values & return\n",
        "        return(np.round(ypred).astype(int))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoPL557KHXka"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY6_QQXOHZHa",
        "outputId": "0e20db14-dfeb-4575-bc0d-5142a0e725a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy: 69.8010%\n",
            "Random Forest Precision: 69.7791%\n",
            "Random Forest Recall: 69.8010%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Convert X_train and X_test to NumPy arrays\n",
        "# X_train = np.array(X_train)\n",
        "# X_test = np.array(X_test)\n",
        "y_train = np.array(y_train).reshape(-1, 1)\n",
        "y_test = np.array(y_test).reshape(-1, 1)\n",
        "\n",
        "random_forest = RandomForest(n_trees ,max_depth, min_samples_split)\n",
        "random_forest.fit(X_train, y_train)\n",
        "y_pred = random_forest.predict(X_test)\n",
        "\n",
        "# Compute accuracy, precision, and recall on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print results\n",
        "print(f\"Random Forest Accuracy: {(accuracy*100):.4f}%\")\n",
        "print(f\"Random Forest Precision: {(precision*100):.4f}%\")\n",
        "print(f\"Random Forest Recall: {(recall*100):.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **4.Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bagging->n_estimators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_estimators: 10, Accuracy: 0.6816\n",
            "n_estimators: 20, Accuracy: 0.6960\n",
            "n_estimators: 50, Accuracy: 0.7055\n",
            "n_estimators: 100, Accuracy: 0.7086\n",
            "n_estimators: 200, Accuracy: 0.7079\n",
            "\n",
            "Best Hyperparameter:\n",
            "n_estimators: 100\n",
            "Accuracy: 0.7086\n"
          ]
        }
      ],
      "source": [
        "# Define the range of n_estimators to test\n",
        "n_estimators_range = [10, 20, 50, 100, 200]  # Values for n_estimators to evaluate\n",
        "\n",
        "# Results storage\n",
        "grid_results = []\n",
        "\n",
        "# Loop through each value of n_estimators\n",
        "for n_estimators in n_estimators_range:\n",
        "    # Initialize the Bagging model\n",
        "    bagging = Bagging(n_estimators=n_estimators)\n",
        "    \n",
        "    # Train the Bagging model\n",
        "    bagging.fit(X_train, y_train)\n",
        "    \n",
        "    # Predict on the test set\n",
        "    y_pred = bagging.predict(X_test)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    # Store the results\n",
        "    grid_results.append({'n_estimators': n_estimators, 'accuracy': accuracy})\n",
        "    print(f\"n_estimators: {n_estimators}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Find the best n_estimators based on accuracy\n",
        "best_result = max(grid_results, key=lambda x: x['accuracy'])\n",
        "\n",
        "# Print the best result\n",
        "print(\"\\nBest Hyperparameter:\")\n",
        "print(f\"n_estimators: {best_result['n_estimators']}\")\n",
        "print(f\"Accuracy: {best_result['accuracy']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Boosting->n_estimators,alpha,max_depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Grid Search...\n",
            "n_estimators: 10, learning_rate: 0.01, max_depth: 1, Accuracy: 0.6845\n",
            "n_estimators: 10, learning_rate: 0.01, max_depth: 3, Accuracy: 0.6980\n",
            "n_estimators: 10, learning_rate: 0.01, max_depth: 5, Accuracy: 0.7061\n",
            "n_estimators: 10, learning_rate: 0.1, max_depth: 1, Accuracy: 0.6982\n",
            "n_estimators: 10, learning_rate: 0.1, max_depth: 3, Accuracy: 0.7080\n",
            "n_estimators: 10, learning_rate: 0.1, max_depth: 5, Accuracy: 0.7150\n",
            "n_estimators: 10, learning_rate: 1, max_depth: 1, Accuracy: 0.7115\n",
            "n_estimators: 10, learning_rate: 1, max_depth: 3, Accuracy: 0.7188\n",
            "n_estimators: 10, learning_rate: 1, max_depth: 5, Accuracy: 0.7160\n",
            "n_estimators: 20, learning_rate: 0.01, max_depth: 1, Accuracy: 0.6859\n",
            "n_estimators: 20, learning_rate: 0.01, max_depth: 3, Accuracy: 0.6995\n",
            "n_estimators: 20, learning_rate: 0.01, max_depth: 5, Accuracy: 0.7072\n",
            "n_estimators: 20, learning_rate: 0.1, max_depth: 1, Accuracy: 0.6995\n",
            "n_estimators: 20, learning_rate: 0.1, max_depth: 3, Accuracy: 0.7136\n",
            "n_estimators: 20, learning_rate: 0.1, max_depth: 5, Accuracy: 0.7187\n",
            "n_estimators: 20, learning_rate: 1, max_depth: 1, Accuracy: 0.7143\n",
            "n_estimators: 20, learning_rate: 1, max_depth: 3, Accuracy: 0.7223\n",
            "n_estimators: 20, learning_rate: 1, max_depth: 5, Accuracy: 0.7156\n",
            "n_estimators: 50, learning_rate: 0.01, max_depth: 1, Accuracy: 0.6984\n",
            "n_estimators: 50, learning_rate: 0.01, max_depth: 3, Accuracy: 0.7007\n",
            "n_estimators: 50, learning_rate: 0.01, max_depth: 5, Accuracy: 0.7112\n",
            "n_estimators: 50, learning_rate: 0.1, max_depth: 1, Accuracy: 0.7076\n",
            "n_estimators: 50, learning_rate: 0.1, max_depth: 3, Accuracy: 0.7185\n",
            "n_estimators: 50, learning_rate: 0.1, max_depth: 5, Accuracy: 0.7217\n",
            "n_estimators: 50, learning_rate: 1, max_depth: 1, Accuracy: 0.7153\n",
            "n_estimators: 50, learning_rate: 1, max_depth: 3, Accuracy: 0.7229\n",
            "n_estimators: 50, learning_rate: 1, max_depth: 5, Accuracy: 0.7151\n",
            "\n",
            "Grid Search Complete!\n",
            "\n",
            "Best Hyperparameters:\n",
            "n_estimators: 50\n",
            "learning_rate: 1\n",
            "max_depth: 3\n",
            "Accuracy: 0.7229\n"
          ]
        }
      ],
      "source": [
        "from itertools import product\n",
        "\n",
        "# Parameter grid definition\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 20, 50,],  # Range for n_estimators\n",
        "    'learning_rate': [0.01, 0.1, 1],  # Range for learning_rate\n",
        "    'max_depth': [1, 3, 5,]  # Range for max_depth\n",
        "}\n",
        "\n",
        "# Results storage\n",
        "grid_results = []\n",
        "\n",
        "# Generate all combinations of hyperparameters using Cartesian product\n",
        "param_combinations = list(product(param_grid['n_estimators'], param_grid['learning_rate'], param_grid['max_depth']))\n",
        "\n",
        "print(\"Starting Grid Search...\")\n",
        "for n_estimators, learning_rate, max_depth in param_combinations:\n",
        "    # Initialize the Boosting model with the current combination of hyperparameters\n",
        "    boosting = Boosting(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth)\n",
        "    boosting.fit(X_train, y_train)\n",
        "    y_pred = boosting.predict(X_test)\n",
        "    \n",
        "    # Evaluate performance using accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    # Store results\n",
        "    grid_results.append({\n",
        "        'n_estimators': n_estimators,\n",
        "        'learning_rate': learning_rate,\n",
        "        'max_depth': max_depth,\n",
        "        'accuracy': accuracy\n",
        "    })\n",
        "    \n",
        "    print(f\"n_estimators: {n_estimators}, learning_rate: {learning_rate}, max_depth: {max_depth}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nGrid Search Complete!\")\n",
        "\n",
        "# Find the best combination based on accuracy\n",
        "best_result = max(grid_results, key=lambda x: x['accuracy'])\n",
        "\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "print(f\"n_estimators: {best_result['n_estimators']}\")\n",
        "print(f\"learning_rate: {best_result['learning_rate']}\")\n",
        "print(f\"max_depth: {best_result['max_depth']}\")\n",
        "print(f\"Accuracy: {best_result['accuracy']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random Forest->n_trees,max_depth,min_samples_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing Grid Search...\n",
            "n_trees: 10, max_depth: 1, min_samples_split: 2, Accuracy: 0.6845, Precision: 0.6911, Recall: 0.6845\n",
            "n_trees: 10, max_depth: 1, min_samples_split: 3, Accuracy: 0.6845, Precision: 0.6911, Recall: 0.6845\n",
            "n_trees: 10, max_depth: 1, min_samples_split: 4, Accuracy: 0.6845, Precision: 0.6911, Recall: 0.6845\n",
            "n_trees: 10, max_depth: 3, min_samples_split: 2, Accuracy: 0.6980, Precision: 0.6978, Recall: 0.6980\n",
            "n_trees: 10, max_depth: 3, min_samples_split: 3, Accuracy: 0.6980, Precision: 0.6978, Recall: 0.6980\n",
            "n_trees: 10, max_depth: 3, min_samples_split: 4, Accuracy: 0.6980, Precision: 0.6978, Recall: 0.6980\n",
            "n_trees: 10, max_depth: 5, min_samples_split: 2, Accuracy: 0.7024, Precision: 0.7085, Recall: 0.7024\n",
            "n_trees: 10, max_depth: 5, min_samples_split: 3, Accuracy: 0.7079, Precision: 0.7108, Recall: 0.7079\n",
            "n_trees: 10, max_depth: 5, min_samples_split: 4, Accuracy: 0.7033, Precision: 0.7082, Recall: 0.7033\n",
            "n_trees: 20, max_depth: 1, min_samples_split: 2, Accuracy: 0.6845, Precision: 0.6911, Recall: 0.6845\n",
            "n_trees: 20, max_depth: 1, min_samples_split: 3, Accuracy: 0.6845, Precision: 0.6911, Recall: 0.6845\n",
            "n_trees: 20, max_depth: 1, min_samples_split: 4, Accuracy: 0.6845, Precision: 0.6911, Recall: 0.6845\n",
            "n_trees: 20, max_depth: 3, min_samples_split: 2, Accuracy: 0.6980, Precision: 0.6978, Recall: 0.6980\n",
            "n_trees: 20, max_depth: 3, min_samples_split: 3, Accuracy: 0.6980, Precision: 0.6978, Recall: 0.6980\n",
            "n_trees: 20, max_depth: 3, min_samples_split: 4, Accuracy: 0.6980, Precision: 0.6978, Recall: 0.6980\n",
            "n_trees: 20, max_depth: 5, min_samples_split: 2, Accuracy: 0.7071, Precision: 0.7103, Recall: 0.7071\n",
            "n_trees: 20, max_depth: 5, min_samples_split: 3, Accuracy: 0.7066, Precision: 0.7103, Recall: 0.7066\n",
            "n_trees: 20, max_depth: 5, min_samples_split: 4, Accuracy: 0.7032, Precision: 0.7102, Recall: 0.7032\n",
            "n_trees: 50, max_depth: 1, min_samples_split: 2, Accuracy: 0.6845, Precision: 0.6911, Recall: 0.6845\n",
            "n_trees: 50, max_depth: 1, min_samples_split: 3, Accuracy: 0.6845, Precision: 0.6911, Recall: 0.6845\n",
            "n_trees: 50, max_depth: 1, min_samples_split: 4, Accuracy: 0.6845, Precision: 0.6911, Recall: 0.6845\n",
            "n_trees: 50, max_depth: 3, min_samples_split: 2, Accuracy: 0.6980, Precision: 0.6978, Recall: 0.6980\n",
            "n_trees: 50, max_depth: 3, min_samples_split: 3, Accuracy: 0.6980, Precision: 0.6978, Recall: 0.6980\n",
            "n_trees: 50, max_depth: 3, min_samples_split: 4, Accuracy: 0.6980, Precision: 0.6978, Recall: 0.6980\n",
            "n_trees: 50, max_depth: 5, min_samples_split: 2, Accuracy: 0.7070, Precision: 0.7109, Recall: 0.7070\n",
            "n_trees: 50, max_depth: 5, min_samples_split: 3, Accuracy: 0.7049, Precision: 0.7108, Recall: 0.7049\n",
            "n_trees: 50, max_depth: 5, min_samples_split: 4, Accuracy: 0.7048, Precision: 0.7098, Recall: 0.7048\n",
            "\n",
            "Best Grid Search Result: {'n_trees': 10, 'max_depth': 5, 'min_samples_split': 3, 'accuracy': 0.7079304282305664, 'precision': np.float64(0.7107588517311686), 'recall': np.float64(0.7079304282305664)}\n"
          ]
        }
      ],
      "source": [
        "from itertools import product\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_trees': [10, 20, 50],\n",
        "    'max_depth': [1, 3, 5],\n",
        "    'min_samples_split': [2, 3, 4,]\n",
        "}\n",
        "\n",
        "# Results storage\n",
        "grid_results = []\n",
        "\n",
        "# Evaluate all combinations\n",
        "print(\"Performing Grid Search...\")\n",
        "for n_trees, max_depth, min_samples_split in product(param_grid['n_trees'], param_grid['max_depth'], param_grid['min_samples_split']):\n",
        "    random_forest = RandomForest(n_trees=n_trees, max_depth=max_depth, min_samples_split=min_samples_split)\n",
        "    random_forest.fit(X_train, y_train)\n",
        "    y_pred = random_forest.predict(X_test)\n",
        "    \n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    \n",
        "    # Store the results\n",
        "    grid_results.append({'n_trees': n_trees, 'max_depth': max_depth, 'min_samples_split': min_samples_split,\n",
        "                         'accuracy': accuracy, 'precision': precision, 'recall': recall})\n",
        "    print(f\"n_trees: {n_trees}, max_depth: {max_depth}, min_samples_split: {min_samples_split}, \"\n",
        "          f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
        "\n",
        "# Find the best parameters\n",
        "best_grid_result = max(grid_results, key=lambda x: x['accuracy'])\n",
        "print(\"\\nBest Grid Search Result:\", best_grid_result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **BEST MODEL**\n",
        "## Boosting\n",
        "- n_estimators: 50\n",
        "- learning_rate: 1\n",
        "- max_depth: 3\n",
        "- __Accuracy: 0.7229__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BEST MODEL Boosting Accuracy: 72.2875%\n",
            "BEST MODEL Boosting Precision: 72.4797%\n",
            "BEST MODEL Boosting Recall: 72.2875%\n"
          ]
        }
      ],
      "source": [
        "boosting = Boosting(50, 1, 3)\n",
        "boosting.fit(X_train, y_train)\n",
        "y_pred = boosting.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(f\"BEST MODEL Boosting Accuracy: {(accuracy*100):.4f}%\")\n",
        "print(f\"BEST MODEL Boosting Precision: {(precision*100):.4f}%\")\n",
        "print(f\"BEST MODEL Boosting Recall: {(recall*100):.4f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
