{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW7FhavoOT6Y"
      },
      "source": [
        "Lab 3\n",
        "1. Ensemble methods\n",
        "    - Bagging\n",
        "    - Boosting\n",
        "    - Random Forests\n",
        "2. Hyperparameter Tuning\n",
        "3. Final System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9TYMTIKOF9R2"
      },
      "outputs": [],
      "source": [
        "from random import seed\n",
        "from random import randrange\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import model_selection\n",
        "from sklearn.base import clone\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "9JmMh4TQO6F0",
        "outputId": "e572e6aa-598f-412b-e92a-67568850ecbb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>systolic</th>\n",
              "      <th>eyesight(left)</th>\n",
              "      <th>hearing(right)</th>\n",
              "      <th>ALT</th>\n",
              "      <th>relaxation</th>\n",
              "      <th>Cholesterol</th>\n",
              "      <th>AST</th>\n",
              "      <th>hearing(left)</th>\n",
              "      <th>smoking</th>\n",
              "      <th>serum creatinine</th>\n",
              "      <th>Gtp</th>\n",
              "      <th>serum creatinine^2</th>\n",
              "      <th>Gtp^2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.981702</td>\n",
              "      <td>-1.257856</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.087326</td>\n",
              "      <td>1.125777</td>\n",
              "      <td>-0.837985</td>\n",
              "      <td>-0.371570</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.597927</td>\n",
              "      <td>-0.295342</td>\n",
              "      <td>0.357517</td>\n",
              "      <td>0.087227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.845852</td>\n",
              "      <td>-1.009169</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.199983</td>\n",
              "      <td>0.681066</td>\n",
              "      <td>-0.063252</td>\n",
              "      <td>0.156700</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.155511</td>\n",
              "      <td>0.025124</td>\n",
              "      <td>1.335205</td>\n",
              "      <td>0.000631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.353802</td>\n",
              "      <td>-1.506543</td>\n",
              "      <td>1</td>\n",
              "      <td>0.250645</td>\n",
              "      <td>-0.208355</td>\n",
              "      <td>-0.626695</td>\n",
              "      <td>0.156700</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.517239</td>\n",
              "      <td>0.537870</td>\n",
              "      <td>0.267536</td>\n",
              "      <td>0.289304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.667465</td>\n",
              "      <td>1.229017</td>\n",
              "      <td>1</td>\n",
              "      <td>0.025331</td>\n",
              "      <td>1.236955</td>\n",
              "      <td>-0.556264</td>\n",
              "      <td>-0.582878</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.597927</td>\n",
              "      <td>-0.199202</td>\n",
              "      <td>0.357517</td>\n",
              "      <td>0.039681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.118125</td>\n",
              "      <td>1.229017</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.763267</td>\n",
              "      <td>-0.097177</td>\n",
              "      <td>-1.436643</td>\n",
              "      <td>-0.688532</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.517239</td>\n",
              "      <td>-0.615808</td>\n",
              "      <td>0.267536</td>\n",
              "      <td>0.379219</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   systolic  eyesight(left)  hearing(right)       ALT  relaxation  \\\n",
              "0  0.981702       -1.257856               1 -0.087326    1.125777   \n",
              "1  1.845852       -1.009169               2 -0.199983    0.681066   \n",
              "2 -0.353802       -1.506543               1  0.250645   -0.208355   \n",
              "3  0.667465        1.229017               1  0.025331    1.236955   \n",
              "4 -0.118125        1.229017               1 -0.763267   -0.097177   \n",
              "\n",
              "   Cholesterol       AST  hearing(left)  smoking  serum creatinine       Gtp  \\\n",
              "0    -0.837985 -0.371570              1        1          0.597927 -0.295342   \n",
              "1    -0.063252  0.156700              2        0          1.155511  0.025124   \n",
              "2    -0.626695  0.156700              1        1         -0.517239  0.537870   \n",
              "3    -0.556264 -0.582878              1        0          0.597927 -0.199202   \n",
              "4    -1.436643 -0.688532              1        1         -0.517239 -0.615808   \n",
              "\n",
              "   serum creatinine^2     Gtp^2  \n",
              "0            0.357517  0.087227  \n",
              "1            1.335205  0.000631  \n",
              "2            0.267536  0.289304  \n",
              "3            0.357517  0.039681  \n",
              "4            0.267536  0.379219  "
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"output.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "K6WzrET4S4xm"
      },
      "outputs": [],
      "source": [
        "X = df.drop('smoking', axis=1)\n",
        "y = df['smoking']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbUVQb2STC4Z",
        "outputId": "af5af86a-3c5d-4c65-93cc-ab7dc0cdc9a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(127404, 12)\n",
            "(31852, 12)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwZVToeeOx0m"
      },
      "source": [
        "# **1. Bagging (Bootstrap Aggregating)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EM_FlMZFYcc"
      },
      "source": [
        "Resources: [Bagging](https://insidelearningmachines.com/build-a-bagging-classifier-in-python/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG8ZDwN_o-Os"
      },
      "source": [
        "Bootstrapping is a statistical method to create sample data without leaving the properties of the actual dataset. The individual samples of data called bootstrap samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGQkJZpAp3ph"
      },
      "source": [
        "## Steps:\n",
        "\n",
        "1. Produce N bootstrap samples on the training data\n",
        "2. Loop through each of the i = 1 -> N bootstrap samples:\n",
        "  - Fit a model to sample i\n",
        "  - Produce the desired predictions with this model.\n",
        "  - Repeat the above two steps, storing the trained models and predictions\n",
        "4. Aggregate the predictions. In the event of having a labelled test set, compare these results with the test dataset labels\n",
        "5. If the results are good, we can deploy our trained ensemble. Input data is provided to the ensemble, and each constituent model produces predictions. These predictions are aggregated to yield a final result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwBXYiDGu0TH"
      },
      "source": [
        "## Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "JoUBDCOeu4X_"
      },
      "outputs": [],
      "source": [
        "n_estimators = 10  # Number of bagging iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "TKl9Jh-wnRtQ"
      },
      "outputs": [],
      "source": [
        "class Bagging():\n",
        "    '''Bagging Classifier from Scratch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_estimators : int\n",
        "        number of bagging iterations\n",
        "    '''\n",
        "\n",
        "    def __init__(self, n_estimators):\n",
        "        self.n_elements = n_estimators\n",
        "        self.models = []\n",
        "\n",
        "    # Private function to make bootstrap samples\n",
        "    def __make_bootstraps(self, data):\n",
        "        # Initialize output dictionary & unique value count\n",
        "        dc = {}\n",
        "        unip = 0\n",
        "        # Get sample size\n",
        "        b_size = data.shape[0]\n",
        "        # Get list of row indexes\n",
        "        idx = [i for i in range(b_size)]\n",
        "        # Loop through the required number of bootstraps\n",
        "        for b in range(self.n_elements):\n",
        "            # Obtain bootstrap samples with replacement\n",
        "            sidx = np.random.choice(idx, replace=True, size=b_size)\n",
        "            b_samp = data[sidx, :]\n",
        "            # Compute number of unique values contained in the bootstrap sample\n",
        "            unip += len(set(sidx))\n",
        "            # Obtain out-of-bag samples for the current bootstrap\n",
        "            oidx = list(set(idx) - set(sidx))\n",
        "            o_samp = np.array([])\n",
        "            if oidx:\n",
        "                o_samp = data[oidx, :]\n",
        "            # Store results\n",
        "            dc['boot_' + str(b)] = {'boot': b_samp, 'test': o_samp}\n",
        "        # Return the bootstrap results\n",
        "        return dc\n",
        "\n",
        "    # Train the ensemble\n",
        "    def fit(self, X_train, y_train, print_metrics=False):\n",
        "      # Convert y_train to a NumPy array\n",
        "      training_data = np.concatenate((X_train, np.array(y_train).reshape(-1, 1)), axis=1)\n",
        "      # Make bootstrap samples\n",
        "      dcBoot = self.__make_bootstraps(training_data)\n",
        "      # Initialize metric arrays\n",
        "      accs = np.array([])\n",
        "      pres = np.array([])\n",
        "      recs = np.array([])\n",
        "      # Iterate through each bootstrap sample & fit a model\n",
        "      cls = DecisionTreeClassifier(class_weight='balanced')\n",
        "      for b in dcBoot:\n",
        "          # Make a clone of the model\n",
        "          model = clone(cls)\n",
        "          # Fit a decision tree classifier to the current sample\n",
        "          model.fit(dcBoot[b]['boot'][:, :-1], dcBoot[b]['boot'][:, -1].reshape(-1, 1))\n",
        "          # Append the fitted model\n",
        "          self.models.append(model)\n",
        "          # Compute the predictions on the out-of-bag test set & compute metrics\n",
        "          if dcBoot[b]['test'].size:\n",
        "              yp = model.predict(dcBoot[b]['test'][:, :-1])\n",
        "              acc = accuracy_score(dcBoot[b]['test'][:, -1], yp)\n",
        "              pre = precision_score(dcBoot[b]['test'][:, -1], yp)\n",
        "              rec = recall_score(dcBoot[b]['test'][:, -1], yp)\n",
        "              # Store the error metrics\n",
        "              accs = np.concatenate((accs, [acc]))\n",
        "              pres = np.concatenate((pres, [pre]))\n",
        "              recs = np.concatenate((recs, [rec]))\n",
        "\n",
        "    # Predict from the ensemble\n",
        "    def predict(self, X):\n",
        "        # Check we've fit the ensemble\n",
        "        if not self.models:\n",
        "            print('You must train the ensemble before making predictions!')\n",
        "            return None\n",
        "        # Loop through each fitted model\n",
        "        predictions = []\n",
        "        for m in self.models:\n",
        "            # Make predictions on the input X\n",
        "            yp = m.predict(X)\n",
        "            # Append predictions to storage list\n",
        "            predictions.append(yp.reshape(-1, 1))\n",
        "        # Compute the ensemble prediction\n",
        "        ypred = np.round(np.mean(np.concatenate(predictions, axis=1), axis=1)).astype(int)\n",
        "        # Return the prediction\n",
        "        return ypred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwSCuUc7CJZ2"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsBg7XsqsdOZ",
        "outputId": "2165812f-4ea0-4b88-c8eb-96805e528373"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bagging Accuracy: 68.7084%\n",
            "Bagging Precision: 68.5250%\n",
            "Bagging Recall: 68.7084%\n"
          ]
        }
      ],
      "source": [
        "# Convert X_train and X_test to NumPy arrays\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "\n",
        "# Train the Bagging model\n",
        "bagging = Bagging(n_estimators=10)\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging.predict(X_test)\n",
        "\n",
        "# Compute accuracy, precision, and recall\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(f\"Bagging Accuracy: {(accuracy*100):.4f}%\")\n",
        "print(f\"Bagging Precision: {(precision*100):.4f}%\")\n",
        "print(f\"Bagging Recall: {(recall*100):.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXQhQEpsOLqC"
      },
      "source": [
        "# **2. Boosting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9BntPcKPIYG"
      },
      "source": [
        "Resources: [Boosting](https://randomrealizations.com/posts/gradient-boosting-multi-class-classification-from-scratch/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w62MzE1-TK2T"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "mRj1WBl3TKEs"
      },
      "outputs": [],
      "source": [
        "n_estimators = 10  # Number of boosting iterations\n",
        "alpha = 0.01  # learning rate\n",
        "max_depth = 3 # maximum tree depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "w-BobGo1eT1Z"
      },
      "outputs": [],
      "source": [
        "class Boosting():\n",
        "    '''Gradient Boosting Classifier from Scratch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_estimators : int\n",
        "        number of boosting iterations\n",
        "\n",
        "    learning_rate : float\n",
        "        learning rate hyperparameter\n",
        "\n",
        "    max_depth : int\n",
        "        maximum tree depth\n",
        "    '''\n",
        "\n",
        "    def __init__(self, n_estimators, learning_rate, max_depth):\n",
        "        self.n_estimators=n_estimators;\n",
        "        self.learning_rate=learning_rate\n",
        "        self.max_depth=max_depth;\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        '''Fit the GBM\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray of size (number observations, number features)\n",
        "            design matrix\n",
        "\n",
        "        y : ndarray of size (number observations,)\n",
        "            integer-encoded target labels in {0,1,...,k-1}\n",
        "        '''\n",
        "\n",
        "        # Flatten y if it is 2D\n",
        "        if len(y.shape) > 1:\n",
        "            y = y.ravel()  # Flatten to 1D\n",
        "\n",
        "        self.n_classes = pd.Series(y).nunique()\n",
        "        y_ohe = self._one_hot_encode_labels(y)\n",
        "\n",
        "        raw_predictions = np.zeros(shape=y_ohe.shape)\n",
        "        probabilities = self._softmax(raw_predictions)\n",
        "        self.boosters = []\n",
        "        for m in range(self.n_estimators):\n",
        "            class_trees = []\n",
        "            for k in range(self.n_classes):\n",
        "                negative_gradients = self._negative_gradients(y_ohe[:, k], probabilities[:, k])\n",
        "                hessians = self._hessians(probabilities[:, k])\n",
        "                tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "                tree.fit(X, negative_gradients)\n",
        "                self._update_terminal_nodes(tree, X, negative_gradients, hessians)\n",
        "                raw_predictions[:, k] += self.learning_rate * tree.predict(X)\n",
        "                probabilities = self._softmax(raw_predictions)\n",
        "                class_trees.append(tree)\n",
        "            self.boosters.append(class_trees)\n",
        "\n",
        "\n",
        "    def _one_hot_encode_labels(self, y):\n",
        "        if isinstance(y, pd.Series): y = y.values\n",
        "        ohe = OneHotEncoder()\n",
        "        y_ohe = ohe.fit_transform(y.reshape(-1, 1)).toarray()\n",
        "        return y_ohe\n",
        "\n",
        "    def _negative_gradients(self, y_ohe, probabilities):\n",
        "        return y_ohe - probabilities\n",
        "\n",
        "    def _hessians(self, probabilities):\n",
        "        return probabilities * (1 - probabilities)\n",
        "\n",
        "    def _softmax(self, raw_predictions):\n",
        "        numerator = np.exp(raw_predictions)\n",
        "        denominator = np.sum(np.exp(raw_predictions), axis=1).reshape(-1, 1)\n",
        "        return numerator / denominator\n",
        "\n",
        "    def _update_terminal_nodes(self, tree, X, negative_gradients, hessians):\n",
        "        '''Update the terminal node predicted values'''\n",
        "        # terminal node id's\n",
        "        leaf_nodes = np.nonzero(tree.tree_.children_left == -1)[0]\n",
        "        # compute leaf for each sample in ``X``.\n",
        "        leaf_node_for_each_sample = tree.apply(X)\n",
        "        for leaf in leaf_nodes:\n",
        "            samples_in_this_leaf = np.where(leaf_node_for_each_sample == leaf)[0]\n",
        "            negative_gradients_in_leaf = negative_gradients.take(samples_in_this_leaf, axis=0)\n",
        "            hessians_in_leaf = hessians.take(samples_in_this_leaf, axis=0)\n",
        "            val = np.sum(negative_gradients_in_leaf) / np.sum(hessians_in_leaf)\n",
        "            tree.tree_.value[leaf, 0, 0] = val\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        '''Generate probability predictions for the given input data.'''\n",
        "        raw_predictions =  np.zeros(shape=(X.shape[0], self.n_classes))\n",
        "        for k in range(self.n_classes):\n",
        "            for booster in self.boosters:\n",
        "                raw_predictions[:, k] +=self.learning_rate * booster[k].predict(X)\n",
        "        probabilities = self._softmax(raw_predictions)\n",
        "        return probabilities\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''Generate predicted labels (as 1-d array)'''\n",
        "        probabilities = self.predict_proba(X)\n",
        "        return np.argmax(probabilities, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PM2fmSuepTz"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rL9au1oeonx",
        "outputId": "4b7b00df-58dc-43ad-f1b2-1c067cd224b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Boosting Accuracy: 69.8010%\n",
            "Boosting Precision: 69.7791%\n",
            "Boosting Recall: 69.8010%\n"
          ]
        }
      ],
      "source": [
        "boosting = Boosting(n_estimators, alpha, max_depth)\n",
        "boosting.fit(X_train, y_train)\n",
        "y_pred = boosting.predict(X_test)\n",
        "\n",
        "# Compute accuracy, precision, and recall on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(f\"Boosting Accuracy: {(accuracy*100):.4f}%\")\n",
        "print(f\"Boosting Precision: {(precision*100):.4f}%\")\n",
        "print(f\"Boosting Recall: {(recall*100):.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8-zh3U4iQeK"
      },
      "source": [
        "## **3. Random Forests**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHXP19BPipE4"
      },
      "source": [
        "Resources: [Random Forest](https://insidelearningmachines.com/build-a-random-forest-in-python/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "PcaBeo_8Hu0g"
      },
      "outputs": [],
      "source": [
        "n_trees = 10 # Number of trees in the forest\n",
        "max_depth = 3 # Maximum depth of the tree\n",
        "min_samples_split = 2 # The minimum number of samples required to split an internal node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Wqr5dpNRiZvR"
      },
      "outputs": [],
      "source": [
        "class RandomForest():\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "class RandomForest():\n",
        "    def __init__(self, n_trees, max_depth, min_samples_split, criterion='gini', balance_class_weights=False):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion = criterion  # Update the parameter name to 'criterion'\n",
        "        self.balance_class_weights = balance_class_weights\n",
        "        self.trees = []\n",
        "\n",
        "    # protected function to obtain the right decision tree\n",
        "    def _make_tree_model(self):\n",
        "        return DecisionTreeClassifier(max_depth=self.max_depth, min_samples_split=self.min_samples_split, criterion=self.criterion, class_weight='balanced' if self.balance_class_weights else None)\n",
        "\n",
        "    # private function to make bootstrap samples\n",
        "    def __make_bootstraps(self,data):\n",
        "        # initialize output dictionary & unique value count\n",
        "        dc = {}\n",
        "        unip = 0\n",
        "        # get sample size\n",
        "        b_size = data.shape[0]\n",
        "        # get list of row indexes\n",
        "        idx = [i for i in range(b_size)]\n",
        "        # loop through the required number of bootstraps\n",
        "        for b in range(self.n_trees):\n",
        "            # obtain boostrap samples with replacement\n",
        "            sidx = np.random.choice(idx,replace=True,size=b_size)\n",
        "            b_samp = data[sidx,:]\n",
        "            #compute number of unique values contained in the bootstrap sample\n",
        "            unip += len(set(sidx))\n",
        "            # obtain out-of-bag samples for the current b\n",
        "            oidx = list(set(idx) - set(sidx))\n",
        "            o_samp = np.array([])\n",
        "            if oidx:\n",
        "                o_samp = data[oidx,:]\n",
        "            #store results\n",
        "            dc['boot_'+str(b)] = {'boot':b_samp,'test':o_samp}\n",
        "        #return the bootstrap results\n",
        "        return(dc)\n",
        "\n",
        "    # protected function to train the ensemble\n",
        "    def _train(self,X_train,y_train):\n",
        "        #package the input data\n",
        "        training_data = np.concatenate((X_train,y_train.reshape(-1,1)),axis=1)\n",
        "        #make bootstrap samples\n",
        "        dcBoot = self.__make_bootstraps(training_data)\n",
        "        #iterate through each bootstrap sample & fit a model ##\n",
        "        tree_m = self._make_tree_model()\n",
        "        dcOob = {}\n",
        "        for b in dcBoot:\n",
        "            # make a clone of the model\n",
        "            model = clone(tree_m)\n",
        "            # fit a decision tree model to the current sample\n",
        "            model.fit(dcBoot[b]['boot'][:,:-1],dcBoot[b]['boot'][:,-1].reshape(-1, 1))\n",
        "            # append the fitted model\n",
        "            self.trees.append(model)\n",
        "            # store the out-of-bag test set for the current bootstrap\n",
        "            if dcBoot[b]['test'].size:\n",
        "                dcOob[b] = dcBoot[b]['test']\n",
        "            else:\n",
        "                dcOob[b] = np.array([])\n",
        "        #return the oob data set\n",
        "        return(dcOob)\n",
        "\n",
        "    # train the ensemble\n",
        "    def fit(self, X_train, y_train,print_metrics=False):\n",
        "        # call the protected training method\n",
        "        dcOob = self._train(X_train,y_train)\n",
        "        # if selected, compute the standard errors and print them\n",
        "        if print_metrics:\n",
        "            # initialise metric arrays\n",
        "            accs = np.array([])\n",
        "            pres = np.array([])\n",
        "            recs = np.array([])\n",
        "            # loop through each bootstrap sample\n",
        "            for b,m in zip(dcOob,self.trees):\n",
        "                # compute the predictions on the out-of-bag test set & compute metrics\n",
        "                if dcOob[b].size:\n",
        "                    yp  = m.predict(dcOob[b][:,:-1])\n",
        "                    acc = accuracy_score(dcOob[b][:,-1],yp)\n",
        "                    pre = precision_score(dcOob[b][:,-1],yp,average='weighted')\n",
        "                    rec = recall_score(dcOob[b][:,-1],yp,average='weighted')\n",
        "\n",
        "    #protected function to predict from the ensemble\n",
        "    def _predict(self,X):\n",
        "        #check we've fit the ensemble\n",
        "        if not self.trees:\n",
        "            print('You must train the ensemble before making predictions!')\n",
        "            return(None)\n",
        "        #loop through each fitted model\n",
        "        predictions = []\n",
        "        for m in self.trees:\n",
        "            #make predictions on the input X\n",
        "            yp = m.predict(X)\n",
        "            #append predictions to storage list\n",
        "            predictions.append(yp.reshape(-1,1))\n",
        "        #compute the ensemble prediction\n",
        "        ypred = np.mean(np.concatenate(predictions,axis=1),axis=1)\n",
        "        #return the prediction\n",
        "        return(ypred)\n",
        "\n",
        "    # predict from the ensemble\n",
        "    def predict(self,X):\n",
        "        # call the protected prediction method\n",
        "        ypred = self._predict(X)\n",
        "        # convert the results into integer values & return\n",
        "        return(np.round(ypred).astype(int))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoPL557KHXka"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY6_QQXOHZHa",
        "outputId": "0e20db14-dfeb-4575-bc0d-5142a0e725a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy: 69.8010%\n",
            "Random Forest Precision: 69.7791%\n",
            "Random Forest Recall: 69.8010%\n"
          ]
        }
      ],
      "source": [
        "# Convert X_train and X_test to NumPy arrays\n",
        "# X_train = np.array(X_train)\n",
        "# X_test = np.array(X_test)\n",
        "y_train = np.array(y_train).reshape(-1, 1)\n",
        "y_test = np.array(y_test).reshape(-1, 1)\n",
        "\n",
        "random_forest = RandomForest(n_trees ,max_depth, min_samples_split)\n",
        "random_forest.fit(X_train, y_train)\n",
        "y_pred = random_forest.predict(X_test)\n",
        "\n",
        "# Compute accuracy, precision, and recall on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print results\n",
        "print(f\"Random Forest Accuracy: {(accuracy*100):.4f}%\")\n",
        "print(f\"Random Forest Precision: {(precision*100):.4f}%\")\n",
        "print(f\"Random Forest Recall: {(recall*100):.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **4.Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bagging->n_estimators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_estimators: 1, Accuracy: 0.6393\n",
            "n_estimators: 2, Accuracy: 0.6312\n",
            "n_estimators: 3, Accuracy: 0.6616\n",
            "n_estimators: 4, Accuracy: 0.6557\n",
            "n_estimators: 5, Accuracy: 0.6731\n",
            "n_estimators: 6, Accuracy: 0.6665\n",
            "n_estimators: 7, Accuracy: 0.6798\n",
            "n_estimators: 8, Accuracy: 0.6750\n",
            "n_estimators: 9, Accuracy: 0.6838\n",
            "n_estimators: 10, Accuracy: 0.6833\n",
            "n_estimators: 11, Accuracy: 0.6872\n",
            "n_estimators: 12, Accuracy: 0.6868\n",
            "n_estimators: 13, Accuracy: 0.6944\n",
            "n_estimators: 14, Accuracy: 0.6905\n",
            "n_estimators: 15, Accuracy: 0.6964\n",
            "n_estimators: 16, Accuracy: 0.6947\n",
            "n_estimators: 17, Accuracy: 0.6935\n",
            "n_estimators: 18, Accuracy: 0.6935\n",
            "n_estimators: 19, Accuracy: 0.6997\n",
            "n_estimators: 20, Accuracy: 0.6937\n",
            "n_estimators: 21, Accuracy: 0.6982\n",
            "n_estimators: 22, Accuracy: 0.6940\n",
            "n_estimators: 23, Accuracy: 0.7000\n",
            "n_estimators: 24, Accuracy: 0.6987\n",
            "n_estimators: 25, Accuracy: 0.7005\n",
            "n_estimators: 26, Accuracy: 0.6969\n",
            "n_estimators: 27, Accuracy: 0.7018\n",
            "n_estimators: 28, Accuracy: 0.6992\n",
            "n_estimators: 29, Accuracy: 0.7023\n"
          ]
        }
      ],
      "source": [
        "# Define the range of n_estimators to test\n",
        "n_estimators_range = range(1, 30)  # From 1 to 20 estimators\n",
        "accuracies = []\n",
        "\n",
        "# Test different values of n_estimators\n",
        "for n_estimators in n_estimators_range:\n",
        "    # Initialize the Bagging model\n",
        "    bagging = Bagging(n_estimators=n_estimators)\n",
        "    \n",
        "    # Train the model\n",
        "    bagging.fit(X_train, y_train)\n",
        "    \n",
        "    # Predict on the test set\n",
        "    y_pred = bagging.predict(X_test)\n",
        "    \n",
        "    # Compute and store accuracy\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"n_estimators: {n_estimators}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Boosting->n_estimators,alpha,max_depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning n_estimators...\n",
            "n_estimators: 1, Accuracy: 0.6980\n",
            "n_estimators: 2, Accuracy: 0.6980\n",
            "n_estimators: 3, Accuracy: 0.6980\n",
            "n_estimators: 4, Accuracy: 0.6980\n",
            "n_estimators: 5, Accuracy: 0.6980\n",
            "n_estimators: 6, Accuracy: 0.6980\n",
            "n_estimators: 7, Accuracy: 0.6980\n",
            "n_estimators: 8, Accuracy: 0.6980\n",
            "n_estimators: 9, Accuracy: 0.6980\n",
            "n_estimators: 10, Accuracy: 0.6980\n",
            "n_estimators: 11, Accuracy: 0.6980\n",
            "n_estimators: 12, Accuracy: 0.6980\n",
            "n_estimators: 13, Accuracy: 0.6980\n",
            "n_estimators: 14, Accuracy: 0.6995\n",
            "n_estimators: 15, Accuracy: 0.6995\n",
            "n_estimators: 16, Accuracy: 0.6995\n",
            "n_estimators: 17, Accuracy: 0.6995\n",
            "n_estimators: 18, Accuracy: 0.6995\n",
            "n_estimators: 19, Accuracy: 0.6995\n",
            "n_estimators: 20, Accuracy: 0.6995\n",
            "n_estimators: 21, Accuracy: 0.6995\n",
            "n_estimators: 22, Accuracy: 0.6995\n",
            "n_estimators: 23, Accuracy: 0.6995\n",
            "n_estimators: 24, Accuracy: 0.6995\n",
            "n_estimators: 25, Accuracy: 0.6995\n",
            "n_estimators: 26, Accuracy: 0.6995\n",
            "n_estimators: 27, Accuracy: 0.6995\n",
            "n_estimators: 28, Accuracy: 0.6995\n",
            "n_estimators: 29, Accuracy: 0.7003\n",
            "n_estimators: 30, Accuracy: 0.7003\n",
            "\n",
            "Finished tuning n_estimators.\n",
            "\n",
            "Tuning learning_rate...\n",
            "learning_rate: 0.01, Accuracy: 0.6980\n",
            "learning_rate: 0.05, Accuracy: 0.7010\n",
            "learning_rate: 0.1, Accuracy: 0.7080\n",
            "learning_rate: 1, Accuracy: 0.7188\n",
            "\n",
            "Finished tuning learning_rate.\n",
            "\n",
            "Tuning max_depth...\n",
            "max_depth: 1, Accuracy: 0.6845\n",
            "max_depth: 2, Accuracy: 0.6980\n",
            "max_depth: 3, Accuracy: 0.6980\n",
            "max_depth: 4, Accuracy: 0.6995\n",
            "max_depth: 5, Accuracy: 0.7061\n",
            "\n",
            "Finished tuning max_depth.\n",
            "\n",
            "Hyperparameter Tuning Results:\n",
            "{'hyperparameter': 'n_estimators', 'value': 1, 'accuracy': 0.6980095441416552}\n",
            "{'hyperparameter': 'n_estimators', 'value': 2, 'accuracy': 0.6980095441416552}\n",
            "{'hyperparameter': 'n_estimators', 'value': 3, 'accuracy': 0.6980095441416552}\n",
            "{'hyperparameter': 'n_estimators', 'value': 4, 'accuracy': 0.6980095441416552}\n",
            "{'hyperparameter': 'n_estimators', 'value': 5, 'accuracy': 0.6980095441416552}\n",
            "{'hyperparameter': 'n_estimators', 'value': 6, 'accuracy': 0.6980095441416552}\n",
            "{'hyperparameter': 'n_estimators', 'value': 7, 'accuracy': 0.6980095441416552}\n",
            "{'hyperparameter': 'n_estimators', 'value': 8, 'accuracy': 0.6980095441416552}\n",
            "{'hyperparameter': 'n_estimators', 'value': 9, 'accuracy': 0.6980095441416552}\n",
            "{'hyperparameter': 'n_estimators', 'value': 10, 'accuracy': 0.6980095441416552}\n",
            "{'hyperparameter': 'n_estimators', 'value': 11, 'accuracy': 0.6980095441416552}\n",
            "{'hyperparameter': 'n_estimators', 'value': 12, 'accuracy': 0.6980095441416552}\n",
            "{'hyperparameter': 'n_estimators', 'value': 13, 'accuracy': 0.6980095441416552}\n",
            "{'hyperparameter': 'n_estimators', 'value': 14, 'accuracy': 0.6994851186738666}\n",
            "{'hyperparameter': 'n_estimators', 'value': 15, 'accuracy': 0.6994851186738666}\n",
            "{'hyperparameter': 'n_estimators', 'value': 16, 'accuracy': 0.6994851186738666}\n",
            "{'hyperparameter': 'n_estimators', 'value': 17, 'accuracy': 0.6994851186738666}\n",
            "{'hyperparameter': 'n_estimators', 'value': 18, 'accuracy': 0.6994851186738666}\n",
            "{'hyperparameter': 'n_estimators', 'value': 19, 'accuracy': 0.6994851186738666}\n",
            "{'hyperparameter': 'n_estimators', 'value': 20, 'accuracy': 0.6994851186738666}\n",
            "{'hyperparameter': 'n_estimators', 'value': 21, 'accuracy': 0.6994851186738666}\n",
            "{'hyperparameter': 'n_estimators', 'value': 22, 'accuracy': 0.6994851186738666}\n",
            "{'hyperparameter': 'n_estimators', 'value': 23, 'accuracy': 0.6994851186738666}\n",
            "{'hyperparameter': 'n_estimators', 'value': 24, 'accuracy': 0.6994851186738666}\n",
            "{'hyperparameter': 'n_estimators', 'value': 25, 'accuracy': 0.6994851186738666}\n",
            "{'hyperparameter': 'n_estimators', 'value': 26, 'accuracy': 0.6994851186738666}\n",
            "{'hyperparameter': 'n_estimators', 'value': 27, 'accuracy': 0.6994851186738666}\n",
            "{'hyperparameter': 'n_estimators', 'value': 28, 'accuracy': 0.6994851186738666}\n",
            "{'hyperparameter': 'n_estimators', 'value': 29, 'accuracy': 0.7003013939470049}\n",
            "{'hyperparameter': 'n_estimators', 'value': 30, 'accuracy': 0.7003013939470049}\n",
            "{'hyperparameter': 'learning_rate', 'value': 0.01, 'accuracy': 0.6980095441416552}\n",
            "{'hyperparameter': 'learning_rate', 'value': 0.05, 'accuracy': 0.7009920884088912}\n",
            "{'hyperparameter': 'learning_rate', 'value': 0.1, 'accuracy': 0.7079618234333794}\n",
            "{'hyperparameter': 'learning_rate', 'value': 1, 'accuracy': 0.7187931684038679}\n",
            "{'hyperparameter': 'max_depth', 'value': 1, 'accuracy': 0.6844782117292477}\n",
            "{'hyperparameter': 'max_depth', 'value': 2, 'accuracy': 0.6980095441416552}\n",
            "{'hyperparameter': 'max_depth', 'value': 3, 'accuracy': 0.6980095441416552}\n",
            "{'hyperparameter': 'max_depth', 'value': 4, 'accuracy': 0.6994851186738666}\n",
            "{'hyperparameter': 'max_depth', 'value': 5, 'accuracy': 0.7060781112645987}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Results storage\n",
        "results = []\n",
        "\n",
        "# Tune `n_estimators` (1 to 30) with fixed learning_rate and max_depth\n",
        "print(\"Tuning n_estimators...\")\n",
        "for n_estimators in range(1, 31):  # Test values from 1 to 30\n",
        "    boosting = Boosting(n_estimators=n_estimators, learning_rate=0.01, max_depth=3)\n",
        "    boosting.fit(X_train, y_train)\n",
        "    y_pred = boosting.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results.append({'hyperparameter': 'n_estimators', 'value': n_estimators, 'accuracy': accuracy})\n",
        "    print(f\"n_estimators: {n_estimators}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nFinished tuning n_estimators.\\n\")\n",
        "\n",
        "# Tune `learning_rate` (0.01, 0.05, 0.1, 1) with fixed n_estimators and max_depth\n",
        "print(\"Tuning learning_rate...\")\n",
        "for learning_rate in [0.01, 0.05, 0.1, 1]:\n",
        "    boosting = Boosting(n_estimators=10, learning_rate=learning_rate, max_depth=3)\n",
        "    boosting.fit(X_train, y_train)\n",
        "    y_pred = boosting.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results.append({'hyperparameter': 'learning_rate', 'value': learning_rate, 'accuracy': accuracy})\n",
        "    print(f\"learning_rate: {learning_rate}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nFinished tuning learning_rate.\\n\")\n",
        "\n",
        "# Tune `max_depth` (1 to 5) with fixed n_estimators and learning_rate\n",
        "print(\"Tuning max_depth...\")\n",
        "for max_depth in range(1, 6):  # Test values from 1 to 5\n",
        "    boosting = Boosting(n_estimators=10, learning_rate=0.01, max_depth=max_depth)\n",
        "    boosting.fit(X_train, y_train)\n",
        "    y_pred = boosting.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results.append({'hyperparameter': 'max_depth', 'value': max_depth, 'accuracy': accuracy})\n",
        "    print(f\"max_depth: {max_depth}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nFinished tuning max_depth.\\n\")\n",
        "\n",
        "# Display all results\n",
        "print(\"Hyperparameter Tuning Results:\")\n",
        "for result in results:\n",
        "    print(result)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
